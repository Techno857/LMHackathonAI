{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9714fe74-81b9-442a-9cb0-824b89d94b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch   \n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Math, Latex\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from nltk.stem import PorterStemmer\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.utils import resample\n",
    "from torch.nn import Dropout\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681e61ed-9356-4b31-922b-5795205cc7f3",
   "metadata": {},
   "source": [
    "# What is Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f32abc3-5e31-4a1c-aaed-f3bdf90fa9de",
   "metadata": {},
   "source": [
    "Natural Language Processing (NLP) is a branch of AI that focuses on the interaction between computers and human language. It combines computational linguistics and machine learning to enable computers to understand, interpret, and generate human language in a way that is both meaningful and useful. \n",
    "\n",
    "Key applications of NLP:\n",
    "- Text Classification\n",
    "- Machine Translation \n",
    "- Chatbots and Virtual Assistants \n",
    "- Sentiment Analyis \n",
    "- Text Summarization\n",
    "- Named Entity Recognition\n",
    "- Speech Recognition\n",
    "- Language Generation \n",
    "- Question Answering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a86dbd-58bf-4b85-8d38-d72a766be721",
   "metadata": {},
   "source": [
    "# Representing Text for Computational Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719f5efc-2e80-4fa8-9478-55c487341471",
   "metadata": {},
   "source": [
    "To enable computers to effectively work with human language, it's essential to convert text into a format they can understand and process. We need to represent language in a numerical or symbolic format that a computer can manipulate. This conversion is a fundamental step in NLP and is essential for various downstream tasks like analysis, translation, and generation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c00abda-ea6b-421b-a66a-7a365a4fc079",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64072bc-c199-4ee1-8866-6c4e027afe44",
   "metadata": {},
   "source": [
    "One of the simplest and most common methods for representing text in a way that computers can understand is the Bag of Words (BoW) model. The idea behind bag-of-words is quite simple and can be summarized as follows:\n",
    "1. Create a vocabulary of unique tokens-for example, words-from the entire set of text documents. \n",
    "2. Construct a feature vector from each document that contains the counts of how often each word occurs in the particular document. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4400d-cba4-4ede-9da7-78e8b88fcfcf",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ac353ba-c379-45de-a3da-b3d53e14de02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab content: \n",
      " {'the': 6, 'sky': 5, 'is': 2, 'blue': 1, 'weather': 8, 'nice': 3, 'and': 0, 'one': 4, 'two': 7}\n",
      "\n",
      " Bag of Words: \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>blue</th>\n",
       "      <th>is</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>sky</th>\n",
       "      <th>the</th>\n",
       "      <th>two</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  blue  is  nice  one  sky  the  two  weather\n",
       "0    0     1   1     0    0    1    1    0        0\n",
       "1    0     0   1     1    0    0    1    0        1\n",
       "2    1     1   2     1    0    1    2    0        1\n",
       "3    2     0   1     0    2    0    0    1        0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = CountVectorizer()\n",
    "\n",
    "documents = np.array(['The sky is blue', 'The weather is nice', 'The sky is blue, and the weather is nice', 'and one and one is two'])\n",
    "bag = count.fit_transform(documents)\n",
    "\n",
    "print(f'Vocab content: \\n {count.vocabulary_}')\n",
    "\n",
    "df = pd.DataFrame(data=bag.toarray(),columns = count.get_feature_names_out())\n",
    "print('\\n Bag of Words: \\n ')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d29a5cf-c079-4390-9d04-22da6e467c24",
   "metadata": {},
   "source": [
    "**Note:** \n",
    "1. We can observe that since the unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectors will mostly consist of zeros, which is why we call them **sparse**.\n",
    "2. For a BoW model, the word or term order in a sentence or document does not matter. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff587e-387d-4230-a46d-8b74f5514abb",
   "metadata": {},
   "source": [
    "### Assessing word relevancy via term frequency-inverse document frequency (tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3f2b5-9522-4b2a-9974-2eeb0788dfdf",
   "metadata": {},
   "source": [
    "We often need to account for frequently occuring words that don't contain useful or discriminatory information. **tfidf** is a useful technique which can be used to downweight these frequently occuring words in the feature vectors. The tf-idf can be defined as the product of the term frequency and the inverse document frequency: <br />\n",
    "\n",
    "*tf-idf(t, d)* = *tf(t,d)* x *(idf(t,d) + 1)*\n",
    "\n",
    "where <br />\n",
    "\n",
    "$ $idf(t,d)$ = log\\frac{1+n_{d}}{1+df(d,t)}$\n",
    "\n",
    "--------- this needs to be cleaned up ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fe2cc14-0dd9-4ded-bbdd-fcbb19c3c550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tfidf: \n",
      " \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>blue</th>\n",
       "      <th>is</th>\n",
       "      <th>nice</th>\n",
       "      <th>one</th>\n",
       "      <th>sky</th>\n",
       "      <th>the</th>\n",
       "      <th>two</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    and  blue    is  nice   one   sky   the   two  weather\n",
       "0  0.00  0.57  0.38  0.00  0.00  0.57  0.46  0.00     0.00\n",
       "1  0.00  0.00  0.38  0.57  0.00  0.00  0.46  0.00     0.57\n",
       "2  0.33  0.33  0.43  0.33  0.00  0.33  0.53  0.00     0.33\n",
       "3  0.57  0.00  0.19  0.00  0.72  0.00  0.00  0.36     0.00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer(use_idf=True, \n",
    "                         norm='l2',\n",
    "                         smooth_idf=True)\n",
    "\n",
    "tfidf_model = tfidf.fit_transform(bag).toarray()\n",
    "\n",
    "df = pd.DataFrame(data=tfidf_model,columns = count.get_feature_names_out())\n",
    "print('\\n Tfidf: \\n ')\n",
    "df.round(decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec4f6675-e65b-4a26-918f-a10855f69211",
   "metadata": {},
   "outputs": [],
   "source": [
    "## EXERCISE: TRY WITH YOUR OWN SENTENCES\n",
    "\n",
    "def dense_vectors(docs):\n",
    "    bag = count.fit_transform(docs)\n",
    "    tfidf_model = tfidf.fit_transform(bag).toarray()\n",
    "    return pd.DataFrame(data=tfidf_model, columns = count.get_feature_names_out())\n",
    "\n",
    "docs = np.array(['', \n",
    "                 '', \n",
    "                 '', \n",
    "                 ''])\n",
    "\n",
    "df = dense_vectors(docs)\n",
    "\n",
    "df.round(decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e16504c-a5a6-4427-8a06-59dbfb56b75f",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829d0b76-ba45-43a1-b556-6d307fe56b37",
   "metadata": {},
   "source": [
    "Text preprocessing in Natural Language Processing (NLP) is a crucial step that involves cleaning and formatting text data before it is used in NLP tasks. The main purpose of text preprocessing is to transform raw text data into a more structured and consistent format that can be easily analyzed and understood by NLP algorithms.\n",
    "\n",
    "Text preprocessing typically involves several steps, including:\n",
    "\n",
    "- Tokenization: This involves breaking down text into individual words or tokens, which can then be analyzed and processed separately.\n",
    "- Stopword removal: This involves removing common words such as \"the,\" \"and,\" and \"a\" that do not contribute much meaning to the text and can be safely removed without losing important information. (self-note to add: no order anyway)\n",
    "- Stemming and lemmatization: These techniques involve reducing words to their base or root form, which can help to reduce the dimensionality of the text data and improve the accuracy of NLP algorithms. (self-note: study and studies shouldnt be considered different tokens)\n",
    "- Noise removal: This involves removing unnecessary characters, symbols, and other forms of noise that can interfere with NLP algorithms.\n",
    "- Text normalization: This involves converting text to a consistent format, such as converting all text to lowercase or uppercase, removing punctuation, and replacing abbreviations with their full forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200e476-e8f4-4956-b0f9-6e597988065d",
   "metadata": {},
   "source": [
    "# Neural Network: Multi-Layer Perceptrons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10adb572-20cd-4dc4-b840-b1cf8a98b475",
   "metadata": {},
   "source": [
    "intro to MLP here\n",
    "something short since we may not have time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "900ba06c-12c2-462b-83bd-0b2285b9d189",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_dataset = pd.read_csv('./data/IMDB Dataset.csv', encoding='utf8')\n",
    "\n",
    "df = imdb_dataset.sample(10000).reset_index(drop=True)\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0)\n",
    "df_train, df_valid = train_test_split(df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b573b9d-3845-4452-abf3-f47970d45f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to preprocess our text\n",
    "def preprocess_text(col, stop_words, stemmer):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    1. col to preprocess\n",
    "    2. list of stop_words\n",
    "    3. word stemmer\n",
    "\n",
    "    steps: tokenize -> lower case ->\n",
    "            remove stop words -> stem\n",
    "    \"\"\"\n",
    "    col = re.sub(r\"[^a-zA-Z0-9 ]\", \"\", col) # only letters and numbers\n",
    "    tokenize = col.split() # split into tokens\n",
    "    lower_case = [word.lower() for word in tokenize] # lowercase for consistent format\n",
    "    remove_stop_words = [word for word in lower_case if word not in stop_words] # remove stop words\n",
    "    stemmed_words = [stemmer.stem(word) for word in remove_stop_words] # stem each word\n",
    "    result = \" \".join(stemmed_words) # join the text back\n",
    "    return result\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stemmer = PorterStemmer()\n",
    "kwargs = {'stop_words': stop_words,\n",
    "           'stemmer': stemmer}\n",
    "df_train['review'] = df_train['review'].apply(lambda x: preprocess_text(x, **kwargs))\n",
    "df_valid['review'] = df_valid['review'].apply(lambda x: preprocess_text(x, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f0882e1-4aea-4707-bd23-502c1ef4b7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train['review']\n",
    "y_train = np.array(df_train['sentiment'])\n",
    "X_val = df_valid['review']\n",
    "y_val = np.array(df_valid['sentiment'])\n",
    "\n",
    "count_vec = CountVectorizer()\n",
    "X_train_counts = count_vec.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "X_val_counts = count_vec.transform(X_val)\n",
    "X_val_tfidf = tfidf_transformer.transform(X_val_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73700c26-96e3-4802-a0a2-36ddbfed1620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "    \n",
    "    \n",
    "X_train = torch.from_numpy(np.array(X_train_tfidf.todense())).float()\n",
    "y_train = torch.from_numpy(y_train)\n",
    "\n",
    "\n",
    "X_val = torch.from_numpy(np.array(X_val_tfidf.todense())).float()\n",
    "y_val = torch.from_numpy(y_val)\n",
    "\n",
    "\n",
    "\n",
    "train_ds = CustomDataset(X_train, y_train)\n",
    "valid_ds = CustomDataset(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23375113-7311-4007-a170-570ea9575f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 128)\n",
    "        self.layer3 = nn.Linear(128, num_classes)\n",
    "        self.drop = nn.Dropout()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "        x = self.drop(x)\n",
    "        x = self.layer2(x)\n",
    "        x = nn.ReLU()(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        x = nn.Softmax(dim=1)(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6459b81-6560-4577-8087-2525998bd965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(NeuralNet, train_ds, valid_ds, epochs, learning_rate, batch_size, model_kwargs):\n",
    "    torch.manual_seed(10)\n",
    "    model = NeuralNet(**model_kwargs).to(Config.device)\n",
    "    print(model)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "    loss_hist_train = []\n",
    "    loss_hist_val = []\n",
    "    acc_hist_train = []\n",
    "    acc_hist_val = []\n",
    "    f1_hist_train = []\n",
    "    f1_hist_val = []\n",
    "\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_hist_loss = 0.0\n",
    "        running_hist_acc = 0.0\n",
    "        running_hist_f1 = 0.0\n",
    "        lens = 0.0\n",
    "        model.train()\n",
    "        for x_batch, y_batch  in tqdm(train_dl):\n",
    "            x_batch, y_batch = x_batch.to(Config.device), y_batch.to(Config.device)\n",
    "            pred = model(x_batch)\n",
    "            loss = criterion(pred, y_batch)\n",
    "            running_hist_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            f1_score_calc = f1_score(to_numpy(torch.argmax(pred, dim=1)), to_numpy(y_batch), average='macro')\n",
    "            running_hist_f1 += f1_score_calc\n",
    "            is_correct = (torch.argmax(pred, dim=1) == y_batch).float()\n",
    "            running_hist_acc += is_correct.sum()\n",
    "            lens += len(x_batch)\n",
    "        running_hist_acc /= lens\n",
    "        running_hist_loss /= lens\n",
    "        running_hist_f1 /= np.ceil(len(train_dl.dataset.X)/batch_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        running_hist_loss_test = 0.0\n",
    "        running_hist_acc_test = 0.0\n",
    "        running_hist_f1_test = 0.0\n",
    "        lens = 0.0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(valid_dl):\n",
    "                x, y = x.to(Config.device), y.to(Config.device)\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "                running_hist_loss_test += loss.item()\n",
    "                f1_score_calc = f1_score(to_numpy(torch.argmax(pred, dim=1)), to_numpy(y), average='macro')\n",
    "                running_hist_f1_test += f1_score_calc\n",
    "                is_correct = (torch.argmax(pred, dim=1) == y)\n",
    "                running_hist_acc_test += is_correct.float().sum()\n",
    "                lens += len(x)\n",
    "\n",
    "            running_hist_loss_test /= lens\n",
    "            running_hist_acc_test /= lens\n",
    "            running_hist_f1_test /= np.ceil(len(valid_dl.dataset.X)/batch_size)\n",
    "        print(f'Predicted Test: {torch.argmax(pred, dim=1).detach().cpu().numpy()}')\n",
    "        print(f'Actual Test:    {y.detach().cpu().numpy()}')\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(\"Train Loss: \\t{:.4f}\".format(running_hist_loss))\n",
    "        print(\"Validation Loss: \\t{:.4f}\".format(running_hist_loss_test))\n",
    "        print(\"Train Accuracy: \\t{:.3f}\".format(running_hist_acc))\n",
    "        print(\"Validation Accuracy: \\t{:.3f}\".format(running_hist_acc_test))\n",
    "        print(\"Train F1: \\t{:.3f}\".format(running_hist_f1))\n",
    "        print(\"Validation F1: \\t{:.3f}\".format(running_hist_f1_test))\n",
    "        print(\"------------------------------\")\n",
    "\n",
    "            # save best model based on accuracy\n",
    "        if (epoch > 1 and running_hist_loss_test < np.min(acc_hist_val)) or epoch == 1:\n",
    "            torch.save(model.state_dict(), \"../models/checkpoint.pt\")\n",
    "\n",
    "        loss_hist_train.append(running_hist_loss)\n",
    "        loss_hist_val.append(running_hist_loss_test)\n",
    "        acc_hist_val.append(running_hist_acc_test.detach().cpu().numpy())\n",
    "        acc_hist_train.append(running_hist_acc.detach().cpu().numpy())\n",
    "        f1_hist_train.append(running_hist_f1)\n",
    "        f1_hist_val.append(running_hist_f1_test)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    # instantiate model and load best weights\n",
    "    model = NeuralNet(**model_kwargs)\n",
    "    model.load_state_dict(torch.load(\"../models/checkpoint.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    return model, loss_hist_train, loss_hist_val, acc_hist_train, acc_hist_val, f1_hist_train, f1_hist_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63870f3e-8e41-4f05-a571-458084234959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    batch_size = 32\n",
    "    learning_rate = 0.0001\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2dfe4c01-a3bd-4314-9376-cd299d984ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(x):\n",
    "    return x.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7a0830c-f4f1-423c-b3a1-4d26bbb99fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNet(\n",
      "  (layer1): Linear(in_features=45504, out_features=1024, bias=True)\n",
      "  (layer2): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (layer3): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (drop): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:13<00:00, 15.97it/s]\n",
      "100%|██████████| 94/94 [00:04<00:00, 20.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Test: [0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0]\n",
      "Actual Test:    [0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0]\n",
      "Epoch 0\n",
      "Train Loss: \t0.0204\n",
      "Validation Loss: \t0.0170\n",
      "Train Accuracy: \t0.714\n",
      "Validation Accuracy: \t0.858\n",
      "Train F1: \t0.653\n",
      "Validation F1: \t0.856\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:13<00:00, 15.83it/s]\n",
      "100%|██████████| 94/94 [00:04<00:00, 20.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Test: [0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0]\n",
      "Actual Test:    [0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0]\n",
      "Epoch 1\n",
      "Train Loss: \t0.0138\n",
      "Validation Loss: \t0.0140\n",
      "Train Accuracy: \t0.923\n",
      "Validation Accuracy: \t0.879\n",
      "Train F1: \t0.921\n",
      "Validation F1: \t0.876\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:11<00:00, 18.31it/s]\n",
      "100%|██████████| 94/94 [00:05<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Test: [0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 0 0 1 1 0]\n",
      "Actual Test:    [0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0]\n",
      "Epoch 2\n",
      "Train Loss: \t0.0113\n",
      "Validation Loss: \t0.0137\n",
      "Train Accuracy: \t0.970\n",
      "Validation Accuracy: \t0.877\n",
      "Train F1: \t0.969\n",
      "Validation F1: \t0.875\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:11<00:00, 18.76it/s]\n",
      "100%|██████████| 94/94 [00:04<00:00, 21.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Test: [0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1]\n",
      "Actual Test:    [0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0]\n",
      "Epoch 3\n",
      "Train Loss: \t0.0105\n",
      "Validation Loss: \t0.0137\n",
      "Train Accuracy: \t0.987\n",
      "Validation Accuracy: \t0.875\n",
      "Train F1: \t0.986\n",
      "Validation F1: \t0.872\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 219/219 [00:11<00:00, 19.22it/s]\n",
      "100%|██████████| 94/94 [00:04<00:00, 23.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Test: [0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 1 0 0 1 1 1]\n",
      "Actual Test:    [0 1 0 0 0 1 0 0 0 1 0 1 0 1 0 1 0 0 1 0 0 1 1 0]\n",
      "Epoch 4\n",
      "Train Loss: \t0.0102\n",
      "Validation Loss: \t0.0137\n",
      "Train Accuracy: \t0.993\n",
      "Validation Accuracy: \t0.872\n",
      "Train F1: \t0.993\n",
      "Validation F1: \t0.870\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'input_size': train_ds.X.shape[1],\n",
    "          'num_classes': 2}\n",
    "model, loss_train, loss_val, acc_train, acc_val, f1_train, f1_val = train_model(NeuralNet, train_ds, valid_ds, Config.epochs, Config.learning_rate, \n",
    "                                                                            Config.batch_size, kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
